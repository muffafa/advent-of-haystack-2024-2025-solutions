{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q2FHwPY-wqJ"
      },
      "source": [
        "# Advent of Haystack: Day 3\n",
        "\n",
        "_Make a copy of this Colab to start_\n",
        "\n",
        "In this challenge, you must help Elf David build a system to answer questions over BBC news dataset. However, to increase recall, your system should be able to generate questions similar to the ones asked.\n",
        "\n",
        "For instance, if Santa asks, `\"How are cybersecurity threats evolving with new technologies?\"` the system should be able to generate similar questions like:\n",
        "\n",
        "- `\"What impact do emerging technologies like AI and IoT have on the landscape of cybersecurity threats?\"`\n",
        "- `\"How are organizations adapting their cybersecurity strategies in response to the evolution of threats driven by technological advancements?\"`\n",
        "- `\"In what ways are cybercriminals leveraging new technologies to enhance their attack methods and tactics?\"`\n",
        "\n",
        "All these questions are similar to the original question, but they are not the same. The idea is that by generating similar questions, you can increase the system's recall, as the system will be able to retrieve more documents that could contain the answer to the original question.\n",
        "For that, you will use a large language model (LLM) to generate alternative similar questions based on the original question.\n",
        "Each of these similar questions will query a document store with news articles; all the documents retrieved by each similar question will be used to compose an answer to the original question.\n",
        "\n",
        "\n",
        "### Components to use:\n",
        "\n",
        "- [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore): to store the news articles.\n",
        "- [`InMemoryEmbeddingRetriever`](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever): to retrieve the documents from the document store.\n",
        "- [`OpenAIGenerator`](https://docs.haystack.deepset.ai/docs/openaigenerator): to instantiate the LLM to generate similar questions and compose an answer to the original question.\n",
        "- [`PromptBuilder`](https://docs.haystack.deepset.ai/docs/promptbuilder): to build the prompts to query the LLM\n",
        "- [`AnswerBuilder`](https://docs.haystack.deepset.ai/docs/answerbuilder): (optional) to build the answers to the original question.\n",
        "- [`SentenceTransformersTextEmbedder`](https://docs.haystack.deepset.ai/docs/sentencetransformerstextembedder): to embed the questions\n",
        "- [`SentenceTransformersDocumentEmbedder`](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder): to embed the news articles\n",
        "- [`DocumentJoiner`](https://docs.haystack.deepset.ai/docs/documentjoiner): to join the documents retrieved by similar query questions\n",
        "\n",
        "### Your task is to build two custom components:\n",
        "\n",
        "- `MultiQueryGenerator`: a custom component that uses an LLM to generate similar questions based on the original question.\n",
        "- `MultiQueryHandler`: a custom component that queries the document store with a set of query questions and collects all the documents\n",
        "\n",
        "**Note:** Feel free to change the models in this challenge and use different model providers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZALZDx-LFebK"
      },
      "source": [
        "### 1) Installation\n",
        "\n",
        "Install packages and the BBC news dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJJ5AT3Z79e3"
      },
      "outputs": [],
      "source": [
        "!pip install haystack-ai\n",
        "!pip install \"sentence-transformers>=3.0.0\"\n",
        "!pip install lazy_imports\n",
        "!pip install -q --upgrade openai # not to get the OpenAI proxies error: https://community.openai.com/t/error-with-openai-1-56-0-client-init-got-an-unexpected-keyword-argument-proxies/1040332/2\n",
        "!wget https://raw.githubusercontent.com/amankharwal/Website-data/master/bbc-news-data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3jwS4A_FmEV"
      },
      "source": [
        "### 2) Enter API keys for LLM\n",
        "If you want to use OpenAI models, save your API key as `OPENAI_API_KEY` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy4xBS6v8n1s",
        "outputId": "303b3c51-2268-4f20-c292-c7e0f63a77b4",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key: ········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85pAAbaPFtZc"
      },
      "source": [
        "### 3) Parse the news dataset and index it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXMwKWzUAEaW"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from typing import List\n",
        "\n",
        "from haystack import Document\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.document_stores.types import DuplicatePolicy\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack import Pipeline\n",
        "\n",
        "def read_documents(file: str) -> List[Document]:\n",
        "    with open(file, \"r\") as file:\n",
        "        reader = csv.reader(file, delimiter=\"\\t\")\n",
        "        next(reader, None)  # skip the headers\n",
        "        docs = []\n",
        "        for row in reader:\n",
        "            category = row[0].strip()\n",
        "            title = row[2].strip()\n",
        "            text = row[3].strip()\n",
        "            docs.append(Document(content=text, meta={\"category\": category, \"title\": title}))\n",
        "\n",
        "    return docs\n",
        "\n",
        "\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "doc_store = InMemoryDocumentStore()\n",
        "\n",
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(\"embedder\", SentenceTransformersDocumentEmbedder(model=embedding_model))\n",
        "indexing_pipeline.add_component(\"writer\", DocumentWriter(doc_store, policy=DuplicatePolicy.OVERWRITE))\n",
        "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
        "\n",
        "documents = read_documents(\"bbc-news-data.csv\")\n",
        "indexing_pipeline.run({\"embedder\":{\"documents\": documents}})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQbBLYVI1cXZ"
      },
      "source": [
        "### 4) Define a custom component `MultiQueryGenerator`\n",
        "This custom component will generate `n` alternatives to each query using a [Generator](https://docs.haystack.deepset.ai/docs/generators) and a [PromptBuilder](https://docs.haystack.deepset.ai/docs/promptbuilder).\n",
        "\n",
        "We have prepared a template for this component, feel free to change it as you wish.\n",
        "\n",
        "**Hint:** You can start with this prompt template 👇  \n",
        "```python\n",
        "template =\n",
        "\"\"\"\n",
        "You are an AI language model assistant. Your task is to generate {{n_variations}} different versions of the\n",
        "given user question by expanding the meaning of it.\n",
        "By generating multiple perspectives on the user question, you will help gather diverse information that will be useful to answer the user question in more comprehensive manner.\n",
        "The generated questions should be concise. Do not just rephrase the question, think about the other topics that are relevent to the user question.\n",
        "\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {{question}}\n",
        "Alternative: questions:\n",
        "\"\"\"\n",
        "```\n",
        "**Hint** If you're using OpenAI models, feel free to use [`n` parameter](https://platform.openai.com/docs/api-reference/chat/create#chat-create-n) of the API and update the prompt accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMtN6v5n1cXZ"
      },
      "outputs": [],
      "source": [
        "from haystack import component\n",
        "\n",
        "@component\n",
        "class MultiQueryGenerator:\n",
        "    def __init__(self):\n",
        "        # You need to define a Generator and a PromptBuilder to pass to that generator\n",
        "        # The template of the PromptBuilder will have two variables:\n",
        "        #    - 'query' a string,\n",
        "        #    - 'n_variations' the number of variations of the query string to generate\n",
        "\n",
        "    @component.output_types(queries=List[str])\n",
        "    def run(self, query: str, n_variations: int = 3):\n",
        "        # You need build a prompt filling in the variables 'query' and 'n_variations'\n",
        "        # This prompt is then pased to a generator, and you need to collect the resul\n",
        "        # You should return a List[str] with the original query, plus, the 'n_variations' generated by the LLM\n",
        "        ...\n",
        "        ...\n",
        "        queries = [query] + [\"query_n1\", \"query_n2\", \"query_n3\"]\n",
        "        return {\"queries\": queries}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyu5dZXH1cXZ"
      },
      "source": [
        "### 5) Define the custom component `MultiQueryHandler`\n",
        "\n",
        "This component will query the document store with multiple questions and collect all the retrieved documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5CM7YsO1cXZ"
      },
      "outputs": [],
      "source": [
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
        "\n",
        "@component\n",
        "class MultiQueryHandler:\n",
        "    def __init__(self, document_store, embedding_model: str):\n",
        "        # Initialize your SentenceTransformersTextEmbedder and InMemoryEmbeddingRetriever here\n",
        "        # Ensure that the embedding model used for indexing is the same one used for querying in SentenceTransformersTextEmbedder\n",
        "\n",
        "    @component.output_types(answers=List[Document])\n",
        "    def run(self, queries: List[str], top_k: int = 3):\n",
        "        # You need to initialize an embedder to embed each query in `queries`\n",
        "        # Each query will be used to retrieve a List[Document] from the document_store\n",
        "        # You then need to pack all those into a single List[Document] and return it\n",
        "        return {\"answers\": documents}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynhmh2Y-1cXZ"
      },
      "source": [
        "### 6) Define the RAG Pipeline with Multi-Query Retrieval\n",
        "\n",
        "Given a question, this RAG pipeline will generate multiple similar questions, query the document store and collect all the retrieved documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuOfXHrP1cXZ",
        "outputId": "2dbbb7c9-825c-41a7-b27c-19faa6537448"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<haystack.core.pipeline.pipeline.Pipeline object at 0x2fbacb470>\n",
              "🚅 Components\n",
              "  - multi_query_generator: MultiQueryGenerator\n",
              "  - multi_query_handler: MultiQueryHandler\n",
              "  - reranker: DocumentJoiner\n",
              "  - prompt_builder: PromptBuilder\n",
              "  - llm: OpenAIGenerator\n",
              "  - answer_builder: AnswerBuilder\n",
              "🛤️ Connections\n",
              "  - multi_query_generator.queries -> multi_query_handler.queries (List[str])\n",
              "  - multi_query_handler.answers -> reranker.documents (List[Document])\n",
              "  - reranker.documents -> prompt_builder.documents (List[Document])\n",
              "  - prompt_builder.prompt -> llm.prompt (str)\n",
              "  - llm.replies -> answer_builder.replies (List[str])\n",
              "  - llm.meta -> answer_builder.meta (List[Dict[str, Any]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from haystack import component, Pipeline, Document\n",
        "from haystack import component, Pipeline, Document\n",
        "from haystack.components.builders import PromptBuilder, AnswerBuilder\n",
        "from haystack.components.embedders import SentenceTransformersTextEmbedder, SentenceTransformersDocumentEmbedder\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "from haystack.components.joiners import DocumentJoiner\n",
        "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.document_stores.types import DuplicatePolicy\n",
        "\n",
        "template = \"\"\"\n",
        "You have to answer the following question based on the given context information only.\n",
        "If the context is empty or just a '\\\\n' answer with None, example: \"None\".\n",
        "\n",
        "Context:\n",
        "{% for document in documents %}\n",
        "    {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{question}}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "pipeline = Pipeline()\n",
        "\n",
        "# add components\n",
        "pipeline.add_component(\"multi_query_generator\", MultiQueryGenerator())\n",
        "pipeline.add_component(\"multi_query_handler\", MultiQueryHandler(document_store=doc_store,embedding_model=embedding_model))\n",
        "pipeline.add_component(\"reranker\", DocumentJoiner(join_mode=\"reciprocal_rank_fusion\"))\n",
        "pipeline.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
        "pipeline.add_component(\"llm\", OpenAIGenerator())\n",
        "pipeline.add_component(\"answer_builder\", AnswerBuilder())\n",
        "\n",
        "# connect components\n",
        "pipeline.connect(\"multi_query_generator.queries\", \"multi_query_handler.queries\")\n",
        "pipeline.connect(\"multi_query_handler.answers\", \"reranker.documents\")\n",
        "pipeline.connect(\"reranker\", \"prompt_builder.documents\")\n",
        "pipeline.connect(\"prompt_builder\", \"llm\")\n",
        "pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
        "pipeline.connect(\"llm.meta\", \"answer_builder.meta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvmVQ85X1cXZ"
      },
      "outputs": [],
      "source": [
        "question = \"Can you give me some suggestions do you have for Christmas presents? Please provide a variety of options.\"\"\n",
        "question = \"What is popular in the music industry today?\"\n",
        "question = \"How are cybersecurity threats evolving with new technologies?\"\n",
        "question = \"What does UK do to prevent piracy in music industry?\"\n",
        "\n",
        "n_variations = 3\n",
        "top_k = 3\n",
        "\n",
        "result = pipeline.run(\n",
        "    {'multi_query_generator':{'query':question, 'n_variations':n_variations},\n",
        "     'multi_query_handler':{'top_k':top_k},\n",
        "     'prompt_builder': {'template_variables': {'question':question}},\n",
        "     'answer_builder':{'query':question}\n",
        "     }, include_outputs_from={\"multi_query_generator\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the answer and the queries used to retrieve more data"
      ],
      "metadata": {
        "id": "h-ixCJwuSGny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTgGCbO91cXZ"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\nQuestions:\\n\")\n",
        "for q in result['multi_query_generator']['queries']:\n",
        "    print(q)\n",
        "print(\"\\n\\nAnswer:\\n\")\n",
        "print(result['answer_builder']['answers'][0].data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}